# Introduction
These are my personal notes while working through the [Dive into Deep Learning](https://d2l.ai) Book

# Exercise Progress
## 2. ğŸŒŸ Preliminaries 
#### 2.1. ğŸŒŸ NDArray âœ…âœ… 
#### 2.2. ğŸŒŸ Pandas âœ…âœ…âœ…âœ…âœ…
#### 2.3. ğŸŒŸ Linear Algebra âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 2.4. ğŸŒŸ Calculus âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 2.5. ğŸŒŸ Automatic Differentiation âœ…âœ…âœ…âœ…âœ…âœ…
#### 2.6. ğŸŒŸ Probability and Statistics âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 2.7. Documentation

## 3. ğŸŒŸ Linear Neural Networks for Regression
#### 3.1. ğŸŒŸ Linear Regression âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 3.2. ğŸŒŸ Object-Oriented Design for Implementation âœ…âœ…
#### 3.3. ğŸŒŸ Synthetic Regression Data âœ…âœ…âœ…âœ… 
#### 3.4. ğŸŒŸ Linear Regression Implementation from Scratch âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 3.5. ğŸŒŸ Concise Implementation of Linear Regression âœ…âœ…âœ…âœ…âœ…
#### 3.6. ğŸŒŸ Generalizations âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 3.7. ğŸŒŸ Weight Decay âœ…âœ…âœ…âœ…âœ…âœ…

## 4. Linear Neural Networks for Classification
#### 4.1. â­ Softmax Regression âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 4.2. â­ The Image Classification Dataset âœ…âœ…âœ…
#### 4.3. â­ The Base Classification Model âœ…âœ…âœ…
#### 4.4. â­ Softmax Regression Implementation from Scratch âœ…âœ…âœ…âœ…âœ…
#### 4.5. â­ Concise Implementation of Softmax Regression âœ…âœ…âœ…âœ…
#### 4.6. â­ Generalization in Classification âœ…âœ…âœ…âœ…
#### 4.7. Environment and Distribution Shift âŒâŒâŒâŒ

## 5. Multilayer Perceptrons
#### 5.1 â­ Multilayer Perceptrons âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 5.2 Implementations of Multilayer Perceptrons âŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…
#### 5.3 Forward Propagation, Backward Propagation, and Computational Graphs
#### 5.4 Numerical Stability and Initialization
#### 5.5 Generalizing in Deep Learning
#### 5.6 Dropout
#### 5.7 Predicting House Prices on Kaggle âœ…âŒâŒâŒâŒ

## 6. ğŸŒŸ Builders' Guide
#### 6.1. ğŸŒŸ Layers and Modules âœ…âœ…âœ…
#### 6.2. ğŸŒŸ Parameter Management âœ…âœ…âœ…
#### 6.3. ğŸŒŸ Parameter Initialization âœ…
#### 6.4. ğŸŒŸ Lazy Initialization âœ…âœ…âœ…
#### 6.5. ğŸŒŸ Custom Layers âœ…âœ…
#### 6.6. ğŸŒŸ File I/O âœ…âœ…âœ…
#### 6.7. ğŸŒŸ GPUs (M1 GPUs are not supported so I don't do these exercises.)

## 7. Convolutional Neural Networks
#### 7.1 From Fully Connected Layers to Convolutions âŒâŒâœ…âœ…âœ…âœ…
#### 7.2 Convolutions for Images 
#### 7.3 Padding and Stride 
#### 7.4 Multiple Input and Multiple Output Channels 
#### 7.5 Pooling 

## 9. Recurrent Neural Networks
#### 9.1 Working with Sequences
#### 9.2 â­ Converting Raw Data into Sequence Data âœ…âœ…âœ…
#### 9.3 Language Models
#### 9.4 Recurrent Neural Networks
#### 9.5 Recurrent Neural Network Implementation from Scratch
#### 9.6 Concise Implementation of Recurrent Neural Networks
#### 9.7 Backpropagation Through Time

## 12. Optimization and Deep Learning
#### 12.1. Optimization and Deep Learning
#### 12.2. Convexity âœ…âŒâœ…âŒâœ…âœ…âœ…âŒ
#### 12.3. Gradient Descent âœ…âŒâœ…âŒâŒ
#### 12.4. Stochastic Gradient Descent
#### 12.5. Minibatch Stochastic Gradient Descent
#### 12.6. Momentum
#### 12.7. Adagrad
#### 12.8. RMSProp
#### 12.9. Adadelta
#### 12.10. Adam
#### 12.11. Learning Rate Scheduling

## 15. Natural Language Processing: Pretraining
#### 15.1 Word embedding (Word2Vec) âŒâŒâŒ
#### 15.2 Approximate Training
#### 15.3 The Dataset for Pretraining Word Embeddings
#### 15.4 Pretraining Word2Vec
#### 15.5 Word Embedding with Global Vectors (GloVe)
#### 15.6 Subword Embedding
#### 15.7 Word Similarity and Analogy
#### 15.8 Bidirectional Encoder Representation from Transformers (BERT)
#### 15.9 The Dataset for pretraining BERT
#### 15.10 Pretraining BERT

## 16 Natural Language Processing: Applications
#### 16.1. Sentiment Analysis and the Dataset
#### 16.2. Sentiment Analysis: Using Recurrent Neural Networks
#### 16.3. Sentiment Analysis: Using Convolutional Neural Networks
#### 16.4. Natural Language Inference and the Dataset
#### 16.5. Natural Language Inference: Using Attention
#### 16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications
#### 16.7. Natural Language Inference: Fine-Tuning BERT

## 17. ğŸŒŸ Reinforcement Learning
#### 17.1 ğŸŒŸ Markov Decision Process (MDP) âœ…âœ…
#### 17.2 ğŸŒŸ Value Iteration âœ…âœ…âœ…âœ…
#### 17.3 ğŸŒŸ Q-Learning âœ…âœ…âœ…

## 18 Gaussian Process
#### 18.1 Introduction to Gaussian Processes âœ…âŒâŒâŒâŒâŒâŒ
#### 18.2 Gaussian Process Inference
#### 18.3 Gaussian Process Inference

## 22. Appendix: Mathematics for Deep Learning
#### 22.1 â­ Geometry and Linear Algebric Operations âœ…âœ…âœ…âœ…âœ…âœ…âœ…
#### 22.2 â­ Eigendecompositions âœ…âœ…âœ…
#### 22.3 â­ Single Variable Calculus âœ…âœ…âœ…âœ…
#### 22.4 â­ Multivariable Calculus âœ…âœ…âœ…
#### 22.5 â­ Integral Calculus âœ…âœ…âœ…âœ…
#### 22.6 â­ Random Variables âœ…âœ…âœ…âœ…
#### 22.7 Maximum Likelihood
#### 22.8 Distributions
#### 22.9 Naive Bayes
#### 22.10 Statistics
#### 22.11 Information Theory

## 23. ğŸŒŸ Appendix: Tools for Deep Learning
#### 23.1. ğŸŒŸ Using Jupyter Notebooks âœ…âœ…âœ…
#### 23.2. ğŸŒŸ Using Amazon SageMaker âœ…âœ…
#### 23.3. ğŸŒŸ Using AWS EC2 Instances âœ…âœ…âœ…
#### 23.4. ğŸŒŸ Using Google Collab âœ…âœ…
#### 23.5. Selecting Servers and GPUs
#### 23.6. ğŸŒŸ Contributing to This Book âœ…âœ…âœ…
#### 23.7. Utility Functions and Classes
#### 23.8. The d2l API Document

# Technical Stuff
## Development Environment
I created a seperate miniconda environment for this repo called "d2l", based on python version `3.9.18`.

The additionally added packages (mostly just linting stuff) are entered in the `requirements.txt`. This is the first time I'm using (mini)conda, next time I'm creating a repo that uses miniconda I'd create a local (mini)conda environment just how I'd do with a .venv file, but now that I works there is no need to reinstall everything just to have it localized. Might change my mind if linking errors appear in the future.

### Modifications of Libraries
Given that I'm the only person working on this repo and there is very likely not going to be any updates to the packages I think it's okay for me to directly modify the packages, mostly to remove things like deprecation and "Work In Progress" type warnings. This has the additional benefit that I don't have to suppress warnings on the module level but instead can deal with them one by one.

#### PyTorch
Inside of ```torch/nn/modules/lazy.py``` I've commented the lazy eval warning in line 180-181.

## Setup Code and Framework specific stuff
I will use [PyTorch](https://github.com/pytorch/pytorch) as the framework while
working through the book.

To get the (PyTorch) files you can run the `get_pytorch_files.sh` shell script.

# Licensing
This repository contains modified versions of the notebooks from the [Dive into Deep Learning](https://d2l.ai/) book, originally created by the D2L team.

The original notebooks are licensed under the [Creative Commons Attribution-ShareAlike 4.0 International Public License](https://creativecommons.org/licenses/by-sa/4.0/), and my modifications are also licensed under the same CC BY-SA 4.0 license.

Attribution:
- The D2L Team
- https://github.com/d2l-ai/d2l-en

My modifications to the notebooks are indicated with comments or other appropriate markers.

Disclaimer: I have no experience with this kind of licensing attribution, so if something is mishandeled or missing, please let me know.
