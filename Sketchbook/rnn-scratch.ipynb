{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_hidden: int, sigma: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs: int = num_inputs\n",
    "        self.num_hiddens: int = num_hidden\n",
    "\n",
    "        self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hidden) * sigma)\n",
    "        self.W_hh = nn.Parameter(torch.randn(num_hidden, num_hidden) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hidden))\n",
    "\n",
    "    def forward(self, inputs: Tensor, state: Tensor = None) -> tuple[list[Tensor], Tensor]:\n",
    "        if state is None:\n",
    "            state = torch.zeros((inputs.shape[1], self.num_hiddens))\n",
    "    \n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            # Fowards the input, does an iteration on the state\n",
    "            # and stores the sum over those and the bias as the new state \n",
    "            state = (X @ self.W_xh) + (state @ self.W_hh) + self.b_h\n",
    "            outputs.append(state)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing that the dimensions are as we expect\n",
    "batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n",
    "rnn = RNNScratch(num_inputs, num_hiddens)\n",
    "X = torch.ones((num_steps, batch_size, num_inputs))\n",
    "outputs, state = rnn(X)\n",
    "\n",
    "assert len(outputs) == num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing that the state tracking works\n",
    "batch_size, num_inputs, num_hidden, num_steps = 2, 16, 32, 2\n",
    "rnn = RNNScratch(num_inputs, num_hidden)\n",
    "X = torch.ones((num_steps, batch_size, num_inputs))\n",
    "outputs, state = rnn(X)\n",
    "\n",
    "X2 = torch.ones((1, 2, 16))\n",
    "_, state2 = rnn(X2)\n",
    "_, state2 = rnn(X2, state2)\n",
    "\n",
    "assert (state == state2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def accuracy(self, Y_hat, Y, averaged=True):\n",
    "        \"\"\"Compute the number of correct predictions.\n",
    "    \n",
    "        Defined in :numref:`sec_classification`\"\"\"\n",
    "        Y_hat = torch.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "        preds = torch.astype(torch.argmax(Y_hat, axis=1), Y.dtype)\n",
    "        compare = torch.astype(preds == torch.reshape(Y, -1), torch.float32)\n",
    "        return torch.reduce_mean(compare) if averaged else compare\n",
    "\n",
    "    def loss(self, Y_hat, Y, averaged=True):\n",
    "        \"\"\"Defined in :numref:`sec_softmax_concise`\"\"\"\n",
    "        Y_hat = torch.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "        Y = torch.reshape(Y, (-1,))\n",
    "        return F.cross_entropy(Y_hat, Y, reduction='mean' if averaged else 'none')\n",
    "\n",
    "    def layer_summary(self, X_shape):\n",
    "        \"\"\"Defined in :numref:`sec_lenet`\"\"\"\n",
    "        X = torch.randn(*X_shape)\n",
    "        for layer in self.net:\n",
    "            X = layer(X)\n",
    "            print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "class RNNLMScratch(nn.Module):\n",
    "    def __init__(self, rnn: RNNScratch, vocab_size, lr = 0.01):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def init_params(self) -> None:\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size\n",
    "            ) * self.rnn.sigma\n",
    "        )\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_set(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
