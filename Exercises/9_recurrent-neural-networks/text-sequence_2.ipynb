{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Estimate the exponent of Zipfian distribution for unigrams, bigrams, and trigrams in this corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](text-sequence_2_1.png)\n",
    "![image](text-sequence_2_2.png)\n",
    "![image](text-sequence_2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "words = text.split()\n",
    "unigram_vocab = Vocab(words, min_freq=5)\n",
    "\n",
    "bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\n",
    "bigram_vocab = Vocab(bigram_tokens, min_freq=5)\n",
    "\n",
    "trigram_tokens = ['--'.join(triple) for triple in zip(\n",
    "    words[:-2], words[1:-1], words[2:])]\n",
    "trigram_vocab = Vocab(trigram_tokens, min_freq=5)\n",
    "\n",
    "unigram_vocab: list[tuple[str, int]] = [(a, b) for (a, b) in unigram_vocab.token_freqs if b >= 0]\n",
    "bigram_vocab: list[tuple[str, int]] = [(a, b) for (a, b) in bigram_vocab.token_freqs if b >= 0]\n",
    "trigram_vocab: list[tuple[str, int]] = [(a, b) for (a, b) in trigram_vocab.token_freqs if b >= 0]\n",
    "\n",
    "vocab_list = [('unigram', unigram_vocab), ('bigram', bigram_vocab), ('trigram', trigram_vocab)]\n",
    "\n",
    "# Function to calculate the sum of squared differences, including initial scale\n",
    "def zipf_cost_with_scale(params, freqs):\n",
    "    alpha, initial_scale = params\n",
    "    N = len(freqs)\n",
    "    rank = np.arange(1, N + 1)\n",
    "    estimated_freqs = initial_scale * (rank ** (-alpha))\n",
    "    return np.sum((freqs - estimated_freqs) ** 2)\n",
    "\n",
    "# Loop through each vocabulary and perform optimization\n",
    "results = {}\n",
    "for name, vocab in vocab_list:\n",
    "    freqs = [freq for token, freq in vocab]\n",
    "    initial_params = [1.0, freqs[0]]  # Initial guesses for alpha and initial_scale\n",
    "    opt_result = minimize(zipf_cost_with_scale, initial_params, args=(freqs,))\n",
    "    results[name] = opt_result.x  # Store the optimized parameters\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_freqs(name, freqs, alpha, initial_scale, filename = None):\n",
    "    y_vals = [initial_scale * ((i + 1) ** (-alpha) if i != 0 else 0.0) for i in range(len(freqs))]\n",
    "    y_vals[0] = y_vals[1]  # Adjust the first value\n",
    "    x_vals = range(len(freqs))\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, freqs, label=f'{name} actual')\n",
    "    plt.plot(x_vals, y_vals, label=f'{name} estimated')\n",
    "    plt.yscale('log')\n",
    "    plt.title(f\"{name.capitalize()}, alpha = {alpha:.4f}\")\n",
    "    plt.legend()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting for each vocabulary\n",
    "for i, (name, opt_params) in enumerate(results.items()):\n",
    "    vocab_freqs = [freq for token, freq in globals()[f'{name}_vocab']]\n",
    "    plot_freqs(name, vocab_freqs, *opt_params, f'../../Exercises/9_recurrent-neural-networks_2_{i}.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
