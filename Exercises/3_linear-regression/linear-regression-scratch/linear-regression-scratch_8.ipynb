{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Try implementing a different loss function, such as the absolute value loss\n",
    "```python\n",
    "(y_hat - d2l.reshape(y, y_hat.shape)).abs().sum()\n",
    "```\n",
    "1. Check what happens for regular data.\n",
    "1. Check whether there is a difference in behavior if you actively perturb some entries, such as $y_5 = 10000$, of $\\mathbf{y}$.\n",
    "1. Can you think of a cheap solution for combining the best aspects of squared loss and absolute value loss?\n",
    "    Hint: how can you avoid really large gradient values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "![image](linear-regression-scratch_8_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)\n",
    "Linear Error\n",
    "\n",
    "![image](linear-regression-scratch_8_2.png)\n",
    "\n",
    "Normal Error (note that the abs value of the error is much higher), the numbers on the y axis are 20000, 40000, 60000, 80000, 100000.\n",
    "\n",
    "![image](linear-regression-scratch_8_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)\n",
    "\n",
    "Could combine both of them, for values close to 0 we use square and for those which are outside from region we use the linear loss. That's basically what Huber's loss function does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
