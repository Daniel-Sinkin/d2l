{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How do you access the gradient of the weights of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is a bit tricky, so to examine the training results you can use this function\n",
    "```python\n",
    "def get_w_b(self):\n",
    "    \"\"\"Defined in :numref:`sec_linear_concise`\"\"\"\n",
    "    return (self.net.weight.data, self.net.bias.data)\n",
    "```\n",
    "which returns the **data** contained in $w$ $b$ as a tuple, explicitly **not** the tensors themselves. For that reason the gradients of the return values are `None`\n",
    "```python\n",
    "w, b = model.get_w_b()\n",
    "assert w.grad is None\n",
    "assert b.grad is None\n",
    "```\n",
    "To get the actual tensors we have to, looking into the `get_w_b` function, omit the .data access:\n",
    "```python\n",
    "w, b = model.net.weight, model.net.bias\n",
    "```\n",
    "and we see:\n",
    "```python\n",
    "w_data, b_data = model.get_w_b()\n",
    "w, b = model.net.weight, model.net.bias\n",
    "assert (w == w_data).all\n",
    "assert (b == b_data).all\n",
    "assert w is not w_data\n",
    "assert b is not b_data\n",
    "print(w.grad, b.grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "> tensor([[-0.0728,  0.1536]]) tensor([-0.1112])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
