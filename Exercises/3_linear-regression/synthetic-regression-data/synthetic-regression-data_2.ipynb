{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Suppose that we want to generate a huge dataset, where both the size of the parameter vector w and the number of examples num_examples are large.\n",
    "1. What happens if we cannot hold all data in memory?\n",
    "2. How would you shuffle the data if it is held on disk? Your task is to design an efficient algorithm that does not require too many random reads or writes. Hint: pseudorandom permutation generators allow you to design a reshuffle without the need to store the permutation table explicitly (Naor and Reingold, 1999)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Would have to store it locally on the disc.\n",
    "- Suppose we have an enormous file storing tensors, we could generate pseudorandom numbers for each tensor, sort the entries with respect to the pseudorandom numbers, this can be done by a type of divide and conquer approach like external merge sort. The now sorted set of data can now be processed by sequentially by reading chunks of the file. Because we used a pseudorandom sorting algorithm it is also possible to reverse the sorting without having to store the permutation table explicitly, similiar to how it'd presented in the linked paper."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
