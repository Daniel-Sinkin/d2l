{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Design a layer that takes an input and computes a tensor reduction, i.e., it returns $y_k = \\sumÂ­_{i, j} W_{ijk} x_i x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do functional reductin to multiply the $n$ inputs to obtain $n^2$ nodes consisting of $x_i x_j$ (could reduce the amount of computation by noting that $x_i = x_j$). Also see Exercise 3.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that (4 * (4 + 1) // 2)=10, which is where the 10 comes from.\n",
      "\n",
      "quadratic_features.weights\n",
      "Parameter containing:\n",
      "tensor([-0.1705, -0.0158, -0.5977, -0.9266,  0.1062,  2.0876, -0.5053,  0.0987,\n",
      "        -1.4225, -0.9952], requires_grad=True)\n",
      "torch.Size([10])\n",
      "\n",
      "linear.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1196,  0.1580, -0.0579, -0.1071,  0.2171,  0.0097, -0.2498, -0.1591,\n",
      "          0.0797,  0.2451],\n",
      "        [ 0.2625, -0.0648, -0.0801,  0.1312,  0.0295,  0.0800, -0.0326,  0.2585,\n",
      "         -0.1507, -0.1237],\n",
      "        [ 0.2545,  0.0711, -0.0905, -0.1273, -0.1730, -0.2179, -0.3147, -0.0760,\n",
      "          0.2336, -0.0460]], requires_grad=True)\n",
      "torch.Size([3, 10])\n",
      "\n",
      "linear.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1557,  0.2839, -0.1932], requires_grad=True)\n",
      "torch.Size([3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "# This is a prime candidate for a type of LazyQuadratic Mixin functionality\n",
    "# as we don't really care how many inputs we have for this layer.\n",
    "class QuadraticFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes in n inputs and has n * (n + 1) / 2 outputs, where the outputs are\n",
    "    x_i * x_j for all 1 <= i < j <= n.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in: int):\n",
    "        super().__init__()\n",
    "        self.num_in = num_in\n",
    "        self.num_interactions = self.num_in * (self.num_in + 1) // 2\n",
    "        self.weights = nn.Parameter(torch.randn(self.num_interactions))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # num_in.shape = n => forward(x).shape = n * (n + 1) / 2\n",
    "        if x.shape != (self.num_in,):\n",
    "            raise ValueError(f\"Expected input shape {self.num_in}, but got {x.shape}\")\n",
    "        return torch.concat([(x[i].view((-1, 1)) * x[i:])[0] for i in range(x.shape[0])]).float()\n",
    "\n",
    "class TensorReducer(nn.Module):\n",
    "    def __init__(self, num_in: int, num_out: int):\n",
    "        super().__init__()\n",
    "        # Ideally we wouldn't even have to specify num_in here, maybe we can\n",
    "        # just skip it? I'll leave it in for now, as I don't know if Modules\n",
    "        # keep track of some kind of internal state for the inputs, maybe\n",
    "        # one has to properly initialize the module with the correct number\n",
    "        # of inputs, or tell it that the forward does a delayed init.\n",
    "        self.quadratic_features = QuadraticFeatures(num_in)\n",
    "        self.linear = nn.LazyLinear(num_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (x_1, ..., x_n) -> (x_i * x_j : 1 <= i < j <= n) -> (y_1, ..., y_m), n = num_in, m = num_out\n",
    "        x = self.quadratic_features(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "my_layer = TensorReducer(4, 3)\n",
    "my_layer(torch.tensor([1.0, 2.0, 3.0, 4.0]))\n",
    "print(f\"Note that {(4 * (4 + 1) // 2)=}, which is where the 10 comes from.\")\n",
    "print()\n",
    "for name, param in my_layer.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(param.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
