{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use the NestMLP model defined in Section 6.1 and access the parameters of the various layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "nestmlp = NestMLP()\n",
    "chimera = nn.Sequential(nestmlp, nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)\n",
    "\n",
    "print(list(nestmlp.state_dict().keys()))\n",
    "print(list(chimera.state_dict().keys()))\n",
    "\n",
    "for (a, _), (b, _) in zip(nestmlp.named_parameters(), chimera.named_parameters()):\n",
    "    assert \"0.\" + a == b\n",
    "\n",
    "for name, param in chimera.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "> torch.Size([2, 10])\n",
    "> torch.Size([2, 10])\n",
    "> torch.Size([2, 10])\n",
    "> tensor(-0.2190, grad_fn=<SumBackward0>)\n",
    "> tensor(0.4553, grad_fn=<SumBackward0>)\n",
    "> ['net.0.weight', 'net.0.bias', 'net.2.weight', 'net.2.bias', 'linear.weight', 'linear.bias']\n",
    "> ['0.net.0.weight', '0.net.0.bias', '0.net.2.weight', '0.net.2.bias', '0.linear.weight', '0.linear.bias', '1.weight', '1.bias', '2.linear.weight', '2.linear.bias']\n",
    "> 0.net.0.weight torch.Size([64, 20])\n",
    "> 0.net.0.bias torch.Size([64])\n",
    "> 0.net.2.weight torch.Size([32, 64])\n",
    "> 0.net.2.bias torch.Size([32])\n",
    "> 0.linear.weight torch.Size([16, 32])\n",
    "> 0.linear.bias torch.Size([16])\n",
    "> 1.weight torch.Size([20, 16])\n",
    "> 1.bias torch.Size([20])\n",
    "> 2.linear.weight torch.Size([20, 20])\n",
    "> 2.linear.bias torch.Size([20])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
