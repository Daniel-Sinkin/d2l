{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement a `cross_entropy` function that follows the definition of the cross-entropy loss function $\\sum_i y_i \\log \\hat{y}_i$.\n",
    "1. Try it out in the code example of this section.\n",
    "1. Why do you think it runs more slowly?\n",
    "1. Should you use it? When would it make sense to?\n",
    "1. What do you need to be careful of? Hint: consider the domain of the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "```python\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -y_hat[range(y_hat.shape[0]), y].log().mean()\n",
    "```\n",
    "\n",
    "(2)\n",
    "It's missing the specific PyTorch optimizations for common operaitons, like combining the entire negation, summation, log taking operation into one fused kernel. I can only run the code on a CPU but my guess is that the change is going to be even more pronounced on a GPU.\n",
    "\n",
    "For example we can parallize the whole operation instead of having to load the data, compute, extract, load, compute, extract, load, compute those three operations back to back. \n",
    "\n",
    "In this sense it's similiar to how $a * x + b$ (or example when computing $b += a * x$) is a single (!) cpu instruction.\n",
    "\n",
    "(3)\n",
    "No, because we have no averging mechanism we have no controll over growth w.r.t. number of samples read, which makes the whole procedure less stable.\n",
    "\n",
    "(4)\n",
    "\n",
    "Have to make sure that we don't have negative $\\hat{y}_i$ values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
