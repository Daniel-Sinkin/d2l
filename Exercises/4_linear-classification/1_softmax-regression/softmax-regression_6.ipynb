{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The function $g(\\mathbf{x}) \\stackrel{\\textrm{def}}{=} \\log \\sum_i \\exp x_i$ is sometimes also referred to as the [log-partition function](https://en.wikipedia.org/wiki/Partition_function_(mathematics)).\n",
    "1. Prove that the function is convex. Hint: to do so, use the fact that the first derivative amounts to the probabilities from the softmax function and show that the second derivative is the variance.\n",
    "1. Show that $g$ is translation invariant, i.e., $g(\\mathbf{x} + b) = g(\\mathbf{x})$.\n",
    "1. What happens if some of the coordinates $x_i$ are very large? What happens if they're all very small?\n",
    "1. Show that if we choose $b = \\mathrm{max}_i x_i$ we end up with a numerically stable implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "We first compute the partial derivative\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_{x_j} g(\\underline{x}) &= \\partial足_{x_j} \\log \\sum_i \\exp(x_i) \\\\\n",
    "&= \\frac{\\partial足_{x_j} \\sum_i \\exp(x_i)}{\\sum_i \\exp(x_i)} \\\\\n",
    "&= \\frac{\\partial足_{x_j} \\exp(x_j)}{\\sum_i \\exp(x_i)} \\\\\n",
    "&= \\frac{\\exp(x_j)}{\\sum_i \\exp(x_i)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "and from this we can compute the second derivative\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_r \\partial_j g(\\underline{x}) &= \\partial足_r \\frac{\\exp(x_j)}{\\sum_i \\exp(x_i)} \\\\\n",
    "&= \\frac{\\left(\\partial_r \\exp{x_j}\\right) \\sum \\exp(x_i) - \\partial_r \\sum_i \\exp(x_i)}{\\left( \\sum_i \\exp(x_i) \\right)^2} \\\\\n",
    "&= \\delta_{rj} \\frac{e^{j}}{\\sum_i \\exp(x_i)} - \\frac{e^r}{\\left(\\sum_i \\exp(x_i) \\right)^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)\n",
    "\n",
    "The way this question is formulated is wrong, I'll assume what they meant is that we can shift the points and that shifts the value, i.e.\n",
    "$$\n",
    "g(\\underline{x} - b) = g(\\underline{x}) - b.\n",
    "$$\n",
    "which I guess could be called something like shift-linearity. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "g(\\underline{x} - b) &= \\log \\sum_i \\exp(x_i - b) \\\\\n",
    "&= \\log \\sum_i \\exp(x_i) \\exp(-b) \\\\\n",
    "&= \\log \\left(\\sum_i \\exp(x_i)\\right) + \\log \\exp(-b) \\\\\n",
    "&= \\log \\left(\\sum_i \\exp(x_i)\\right) - b.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)\n",
    "\n",
    "If some coordinates are very large then $\\exp(x_i)$ would be much larger than the other $\\exp$ values and would drown out the other values.\n",
    "\n",
    "Them being small shouldn't make any problems as the individual $\\exp$ terms would be around $1$ and so the total would be around $\\log(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4)\n",
    "\n",
    "Also it's not too difficult to have an overflow on an individual large exp, because we have to compute it before we can take the log. Subtracting (and I think that's what should be done instead of adding) the max over all the values would result in all of them being $\\geq 0$ a so better behaved. Subtracting the $\\max$ value at the end is not a problem as that has no exponential applied to it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
