{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Given a multiclass classification loss, denoting by $l(y,y')$ the penalty of estimating $y'$ when we see $y$ and given a probabilty $p(y \\mid x)$, formulate the rule for an optimal selection of $y'$. Hint: express the expected loss, using $l$ and $p(y \\mid x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Generated Solution\n",
    "Disclaimer: The following was generated by Claude3, as I didn't understand the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected loss for predicting $y'$ when the true label is $y$ is given by:\n",
    "\n",
    "$$L(y') = \\mathbb{E}_{y \\sim p(y \\mid x)}[l(y, y')] \\tag{1}$$\n",
    "\n",
    "Which can be expanded as:\n",
    "\n",
    "$$L(y') = \\sum_{y} l(y, y') p(y \\mid x) \\tag{2}$$\n",
    "\n",
    "To find the optimal prediction $y'$, we want to minimize this expected loss $L(y')$ over all possible predictions $y'$:\n",
    "\n",
    "$$y'_{opt} = \\text{arg}\\min_{y'} L(y') = \\text{arg}\\min_{y'} \\sum_{y} l(y, y') p(y \\mid x) \\tag{3}$$\n",
    "\n",
    "Thus, the optimal prediction rule is:\n",
    "\n",
    "$$y'_{opt} = \\text{arg}\\min_{y'} \\sum_{y} l(y, y') p(y \\mid x) \\tag{4}$$\n",
    "\n",
    "If the loss function is the 0-1 loss:\n",
    "\n",
    "$$l(y, y') = \\mathbb{1}_{\\{y \\neq y'\\}} \\tag{5}$$\n",
    "\n",
    "Then the optimal prediction is the mode of $p(y \\mid x)$:\n",
    "\n",
    "$$y'_{opt} = \\text{arg}\\max_{y'} p(y' \\mid x) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "(This is my own work, trying to understand what was generated above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of predicting $y'$ is defined by $L(y')$, which is given as the `expected value` over `l(y, y')` where y is sampled from the distribution $p(y|x)$.\n",
    "\n",
    "I think this is highly unfortunate (but I think standard) notation as we reuse $y$ in $y ~ p(y | x)$.\n",
    "\n",
    "Assuming and input $x$ we get some different likelihoods for obtaining a particular $y$, this mapping of y to the likelihood is our distribution.\n",
    "\n",
    "Let $Y$ be the random variable of this sampling, i.e., whose underlying distribution is $p(y | x)$. Sampling an object $\\tilde{y}$ from $Y$ is denoted by $\\tilde{y} \\sim p(y | x)$.\n",
    "\n",
    "What we are then interested in is the expected value of $f(Y)$ where\n",
    "$$\n",
    "\\tilde{y} \\sim p(y | x) : f(\\tilde{y}) = \\ell(y, y').\n",
    "$$\n",
    "The expected value can then be computed as\n",
    "$$\n",
    "E(f(Y)) = \\sum_{\\tilde{y}} \\ell(\\tilde{y}, y') p(\\tilde{y}|x)\n",
    "$$\n",
    "With this we can find an optimal choice as follows:\n",
    "$$\n",
    "y'_{opt} = \\argmin_{y'} L(y') = \\argmin_{y'} \\sum_{\\tilde{y}} \\ell(\\tilde{y}, y') p(\\tilde{y}|x)\n",
    "$$\n",
    "Now suppose our error is $\\ell(\\tilde{y}, y') = \\mathbb­{1}_{\\{y \\neq y'\\}}$, the so-called 1-0 loss, then we have\n",
    "$$\n",
    "y'­_{opt} = \\argmin_{y'} \\sum_{\\tilde{y}} \\ell(\\tilde{y}, y') p(\\tilde{y}|x) = \\argmax_{\\tilde{y}} p(\\tilde{y}|x).\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
