{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Is there a difference between weight initializations of the network? Does it matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you could start with random weight inits and do gradient descent on $-\\operatorname{loss}$ and make the loss worse and worse, then roughly speaking you'd need the same amount of steps to get to where you were before, if that was 1000 steps then you've just wasted 1000 training steps on nothing just by choosing a terrible starting position.\n",
    "\n",
    "If we have degenerately large weights we could even get numerical problems, but that's probably a pure theoretical thing.\n",
    "\n",
    "Depending on \"how convex\" our optimization problem is we could miss global optima by having different weights (i.e. have different starting positions)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
