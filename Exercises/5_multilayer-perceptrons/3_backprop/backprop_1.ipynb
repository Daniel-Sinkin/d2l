{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Assume that the inputs $\\mathbb{X}$ to some scalar function $f$ are $n \\times m$ matrices. What is the dimensionality of the gradient of $f$ with respect to $\\mathbb{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can write\n",
    "$$\n",
    "f: \\mathbb{R}^{n \\times m} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "and its gradient has the same dimensionality as the domain:\n",
    "$$\n",
    "Df(x)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} f(x) \\in \\mathbb{R}^{n \\times m}.\n",
    "$$\n",
    "Note that this is also the reason why we have to reduce the \"gradient\" during backpropagation if we'd get a general jacobian. The way that PyTorch stores the gradient is as a 1 dimensional tensor which has as many entries as the underlying tensor."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
