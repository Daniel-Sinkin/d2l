{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Assume that the computational graph is too large for your GPU.\n",
    "1. Can you partition it over more than one GPU?\n",
    "1. What are the advantages and disadvantages over training on a smaller minibatch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "Yeah, as we accumulate the gradients anyway we can split up the computation graph into multiple parts.\n",
    "\n",
    "(2)\n",
    "\n",
    "Faster epochs, less memory footprint every time. Each iteration is less indicative of the total pattern."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
