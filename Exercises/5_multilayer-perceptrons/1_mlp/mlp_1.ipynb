{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Show that adding layers to a linear deep network, i.e., a network without nonlinearity $\\sigma$ can never increase the expressive power of the network. Give an example where it actively reduces it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation\n",
    "First note that we can write the $ith$ entry of a matrix-vector product as follows:\n",
    "$$\n",
    "(Bx)_i = \\sum_j B_{ij} x_j\n",
    "$$\n",
    "and so\n",
    "$$\n",
    "(ABx)_r = \\sum_i A_{ri} (Bx)_i = \\sum_k A_{rk} \\sum_j B_{ij} x_j = \\sum_k \\underbrace{A_{ri} B_{ij}}_{=: C_{rj}} x_j\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "AB = C\n",
    "$$\n",
    "is again a linear map.\n",
    "## On dimensions\n",
    "$x \\in \\mathbb{R}^n, B \\in \\mathbb{R}^{m \\times n}, A \\in \\mathbb{R}^{s \\times m}$ so that\n",
    "$$\n",
    "C = AB \\in \\mathbb{R}^{s \\times n}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Cx = ABx \\in \\mathbb{R}^s.\n",
    "$$\n",
    "\n",
    "It lowers the expressiveness if the other matrix doesn't have full rank but the inner one does. Most extreme case would be $B$ being the identity matrix and $A$ being the $0$ matrix."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
