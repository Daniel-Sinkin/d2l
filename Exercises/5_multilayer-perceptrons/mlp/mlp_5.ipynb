{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sigmoid and tanh are very similar.\n",
    "1. Show that $\\tanh(x) + 1 = 2 \\operatorname{sigmoid}(2x)$.\n",
    "2. Prove that the function classes parametrized by both nonlinearities are identical. Hint: affine layers have bias terms, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tanh(x) + 1 &= \\frac{e^x - e^{-x}}{e^x + e^{-x}} + 1 \\\\\n",
    "&= \\frac{e^x - e^{-x} + e^{-x} + e^x}{e^{-x} + e^x} \\\\\n",
    "&= \\frac{2e^x}{e^{-x} + e^x} \\\\\n",
    "2 \\operatorname{sigmoid}(2x) &= \\frac{2}{1 + e^{-2x}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Those two are equal if and only if\n",
    "$$\n",
    "e^{-x} + e^x = e^x(1 + e^{-2x})\n",
    "$$\n",
    "is true by multiplying both sides by the corresponding denomiators, and this obviously holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(2)\n",
    "\n",
    "The computation in (1) shows that by scaling all weights by factor 2 (the interior 2), by multiplying everything before sending it off to the enxt layer (the exterior 2) and offsetting everything by $-1$ we get an equivalent MLP using the sigmoid function instead of tanh and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
